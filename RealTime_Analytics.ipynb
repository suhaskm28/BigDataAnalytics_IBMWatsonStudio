{
  "metadata": {
    "kernelspec": {
      "name": "python311",
      "display_name": "Python 3.11 with Spark",
      "language": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Real-Time Analytics - IBM Cloud Object Storage & Spark Integration\n",
        "\n",
        "## **1Ô∏è IBM Cloud Object Storage (COS) Configuration**\n",
        "This section defines the necessary parameters to connect to IBM Cloud Object Storage:\n",
        "\n",
        "- **`SERVICE_NAME`**: Logical service name for Spark to recognize the COS instance.\n",
        "- **`ACCESS_KEY`**: Your IBM Cloud Object Storage access key.\n",
        "- **`SECRET_KEY`**: Your IBM Cloud Object Storage secret key.\n",
        "- **`ENDPOINT`**: The region-specific endpoint URL for COS (Frankfurt: `eu-de`).\n",
        "- **`COS_BUCKET`**: The bucket where processed data is stored.\n",
        "- **`COS_FILE`**: The name of the processed dataset file.\n"
      ],
      "metadata": {
        "id": "taHuH4JnLnTN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SERVICE_NAME = \"mycos\"  # logical service name\n",
        "ACCESS_KEY = \"access_key\"         # Copy from your new service credentials\n",
        "SECRET_KEY = \"secret_key\"\n",
        "ENDPOINT = \"s3.eu-de.cloud-object-storage.appdomain.cloud\"  # Frankfurt region endpoint\n",
        "COS_BUCKET = \"processed-data-bucket\"\n",
        "COS_FILE = \"processed_customer_purchase_behavior.csv\""
      ],
      "metadata": {
        "msg_id": "b2783017-da6b-4349-a1ae-0e94167894eb",
        "id": "o5yBIc9kKY6f"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **üîß Configuration Details**\n",
        "- **`appName`**: `\"Real_Time_Analytics\"` - Sets the name of the Spark application.\n",
        "- **`stocator.scheme.list`**: `\"cos\"` - Enables Spark to recognize COS as a valid storage scheme.\n",
        "- **`fs.cos.impl`**: `\"com.ibm.stocator.fs.ObjectStoreFileSystem\"` - Registers Stocator as the file system handler for COS.\n",
        "- **Authentication Credentials**:\n",
        "  - **`access.key`**: IBM COS Access Key.\n",
        "  - **`secret.key`**: IBM COS Secret Key.\n",
        "  - **`endpoint`**: The regional endpoint (Frankfurt: `s3.eu-de.cloud-object-storage.appdomain.cloud`).\n",
        "- **`getOrCreate()`**: Ensures a new Spark session is created if not already running."
      ],
      "metadata": {
        "id": "bNvgVU58MdkE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Spark Session with basic Stocator config using the logical service name\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Real_Time_Analytics\") \\\n",
        "    .config(\"spark.hadoop.fs.stocator.scheme.list\", \"cos\") \\\n",
        "    .config(\"spark.hadoop.fs.cos.impl\", \"com.ibm.stocator.fs.ObjectStoreFileSystem\") \\\n",
        "    .config(f\"spark.hadoop.fs.cos.{SERVICE_NAME}.access.key\", ACCESS_KEY) \\\n",
        "    .config(f\"spark.hadoop.fs.cos.{SERVICE_NAME}.secret.key\", SECRET_KEY) \\\n",
        "    .config(f\"spark.hadoop.fs.cos.{SERVICE_NAME}.endpoint\", f\"https://{ENDPOINT}\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "print(\"‚úÖ Spark Session Initialized with COS Service Name:\", SERVICE_NAME)"
      ],
      "metadata": {
        "msg_id": "a60c7153-9a48-467b-90c6-06b05f102972",
        "id": "2FR4GaPRKY6j",
        "outputId": "d4db575c-b0cb-4d02-dfaf-1b5db2a5db5a"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "‚úÖ Spark Session Initialized with COS Service Name: mycos\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. Configuring Hadoop for IBM Cloud Object Storage**\n",
        "In this step, we explicitly set the **Hadoop configuration** to allow Spark to authenticate and interact with **IBM Cloud Object Storage (COS)**.\n",
        "\n",
        "While the previous Spark session initialization provides access to COS, explicitly setting the **Hadoop configuration** ensures:\n",
        "- **Seamless integration** with Spark's file system operations.\n",
        "- **Improved stability** for reading and writing large datasets.\n",
        "- **Persistent access** across different Spark jobs.\n",
        "\n",
        "---\n",
        "\n",
        "## ** Hadoop Configuration Parameters**\n",
        "- **`fs.cos.{SERVICE_NAME}.access.key`**: Sets the access key for authentication.\n",
        "- **`fs.cos.{SERVICE_NAME}.secret.key`**: Sets the secret key for secure access.\n",
        "- **`fs.cos.{SERVICE_NAME}.endpoint`**: Defines the regional endpoint for object storage access."
      ],
      "metadata": {
        "id": "n7xTn0VVMquv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Explicitly set the Hadoop configuration for COS credentials\n",
        "hadoopConf = spark._jsc.hadoopConfiguration()\n",
        "hadoopConf.set(f\"fs.cos.{SERVICE_NAME}.access.key\", ACCESS_KEY)\n",
        "hadoopConf.set(f\"fs.cos.{SERVICE_NAME}.secret.key\", SECRET_KEY)\n",
        "hadoopConf.set(f\"fs.cos.{SERVICE_NAME}.endpoint\", f\"https://{ENDPOINT}\")"
      ],
      "metadata": {
        "msg_id": "c6e515a9-6bde-4c73-9318-50cd767a2d55",
        "id": "VCetnlX7KY6p"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Construct COS URL\n",
        "\n",
        "The Cloud Object Storage (COS) URL is built using the logical service name and bucket details."
      ],
      "metadata": {
        "id": "CmnmqZ-lNIa_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the COS URL using the logical service name:\n",
        "cos_url = f\"cos://{COS_BUCKET}.{SERVICE_NAME}/{COS_FILE}\"\n",
        "print(\"Using COS URL:\", cos_url)"
      ],
      "metadata": {
        "msg_id": "3b0d81bb-043b-48eb-9ed1-fd846c576c94",
        "id": "0r4BY2p6KY6r",
        "outputId": "63b807af-9863-46e6-edab-f251bacb5aa8"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Using COS URL: cos://processed-data-bucket.mycos/processed_customer_purchase_behavior.csv\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configure Spark with COS Credentials\n",
        "\n",
        "Spark configurations are updated to enable access to IBM Cloud Object Storage."
      ],
      "metadata": {
        "id": "aSHUjm2FNSNR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.conf.set(f\"spark.hadoop.fs.cos.{COS_BUCKET}.access.key\", ACCESS_KEY)\n",
        "spark.conf.set(f\"spark.hadoop.fs.cos.{COS_BUCKET}.secret.key\", SECRET_KEY)\n",
        "spark.conf.set(f\"spark.hadoop.fs.cos.{COS_BUCKET}.endpoint\", f\"https://{ENDPOINT}\")\n",
        "\n",
        "print(\"‚úÖ Spark Configuration Updated with COS Credentials\")\n"
      ],
      "metadata": {
        "msg_id": "e69975ad-6e82-4ff1-995e-714251ac1cc4",
        "id": "Fzzx31KwKY6r",
        "outputId": "d263eac3-11ba-4fc5-f271-caeae4940b71"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "‚úÖ Spark Configuration Updated with COS Credentials\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Schema and Load Data from COS\n",
        "\n",
        "The dataset schema is defined using `StructType` and `StructField`. The processed data is then loaded from IBM Cloud Object Storage using the specified schema."
      ],
      "metadata": {
        "id": "72_MLQQGOJ3r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the schema for your dataset\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
        "\n",
        "schema = StructType([\n",
        "    StructField(\"Customer ID\", StringType(), True),\n",
        "    StructField(\"Age\", IntegerType(), True),\n",
        "    StructField(\"Gender\", StringType(), True),\n",
        "    StructField(\"Item Purchased\", StringType(), True),\n",
        "    StructField(\"Category\", StringType(), True),\n",
        "    StructField(\"Purchase Amount (USD)\", DoubleType(), True),\n",
        "    StructField(\"Location\", StringType(), True),\n",
        "    StructField(\"Size\", StringType(), True),\n",
        "    StructField(\"Color\", StringType(), True),\n",
        "    StructField(\"Season\", StringType(), True),\n",
        "    StructField(\"Review Rating\", DoubleType(), True),\n",
        "    StructField(\"Subscription Status\", IntegerType(), True),\n",
        "    StructField(\"Payment Method\", StringType(), True),\n",
        "    StructField(\"Shipping Type\", StringType(), True),\n",
        "    StructField(\"Discount Applied\", IntegerType(), True),\n",
        "    StructField(\"Promo Code Used\", IntegerType(), True),\n",
        "    StructField(\"Previous Purchases\", IntegerType(), True),\n",
        "    StructField(\"Preferred Payment Method\", StringType(), True),\n",
        "    StructField(\"Frequency of Purchases\", StringType(), True),\n",
        "    StructField(\"High Value Customer\", StringType(), True)\n",
        "])\n",
        "\n",
        "# Read the data from COS using Stocator\n",
        "processed_df = spark.read \\\n",
        "    .option(\"header\", \"true\") \\\n",
        "    .schema(schema) \\\n",
        "    .csv(cos_url)\n",
        "\n",
        "print(\"‚úÖ Processed Data Loaded Successfully!\")\n",
        "processed_df.show(5)"
      ],
      "metadata": {
        "msg_id": "8043e3a3-a4a3-4eb4-b707-0542dd37d787",
        "id": "zLM9PMsVKY6s"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Write Batch Data to a Temporary Directory"
      ],
      "metadata": {
        "id": "IqEe30LGKY6s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Define a temporary directory to store CSV files (adjust path if necessary)\n",
        "temp_dir = \"/tmp/stream_data\"\n",
        "os.makedirs(temp_dir, exist_ok=True)\n",
        "\n",
        "# Write the processed DataFrame to this directory as CSV files (overwrite existing)\n",
        "processed_df.write.mode(\"overwrite\").option(\"header\", \"true\").csv(temp_dir)\n",
        "print(\"‚úÖ Processed data written to:\", temp_dir)\n"
      ],
      "metadata": {
        "msg_id": "1d194573-8a2e-49eb-9e87-b9a229313934",
        "id": "0DGgLm7JKY6t",
        "outputId": "628dbfe4-12af-4fa8-b4a2-b21a9b26803d"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "‚úÖ Processed data written to: /tmp/stream_data\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Schema and Read Streaming Data\n",
        "\n",
        "The schema is defined using `StructType` and `StructField`. A streaming DataFrame is created by reading data from the temporary directory with a specified schema."
      ],
      "metadata": {
        "id": "mnWQF0yoKY6t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
        "\n",
        "# Use the same schema as before\n",
        "schema = StructType([\n",
        "    StructField(\"Customer ID\", StringType(), True),\n",
        "    StructField(\"Age\", IntegerType(), True),\n",
        "    StructField(\"Gender\", StringType(), True),\n",
        "    StructField(\"Item Purchased\", StringType(), True),\n",
        "    StructField(\"Category\", StringType(), True),\n",
        "    StructField(\"Purchase Amount (USD)\", DoubleType(), True),\n",
        "    StructField(\"Location\", StringType(), True),\n",
        "    StructField(\"Size\", StringType(), True),\n",
        "    StructField(\"Color\", StringType(), True),\n",
        "    StructField(\"Season\", StringType(), True),\n",
        "    StructField(\"Review Rating\", DoubleType(), True),\n",
        "    StructField(\"Subscription Status\", IntegerType(), True),\n",
        "    StructField(\"Payment Method\", StringType(), True),\n",
        "    StructField(\"Shipping Type\", StringType(), True),\n",
        "    StructField(\"Discount Applied\", IntegerType(), True),\n",
        "    StructField(\"Promo Code Used\", IntegerType(), True),\n",
        "    StructField(\"Previous Purchases\", IntegerType(), True),\n",
        "    StructField(\"Preferred Payment Method\", StringType(), True),\n",
        "    StructField(\"Frequency of Purchases\", StringType(), True),\n",
        "    StructField(\"High Value Customer\", StringType(), True)\n",
        "])\n",
        "\n",
        "# Read streaming data from the temporary directory\n",
        "streaming_df = spark.readStream \\\n",
        "    .option(\"header\", \"true\") \\\n",
        "    .option(\"maxFilesPerTrigger\", 1) \\\n",
        "    .schema(schema) \\\n",
        "    .csv(temp_dir)\n",
        "\n",
        "print(\"‚úÖ Streaming DataFrame created from directory\")\n"
      ],
      "metadata": {
        "msg_id": "fba84301-195c-4d96-8cb5-1349474baecc",
        "id": "GvoY6rF6KY6u",
        "outputId": "9bf9245e-b902-472f-d2cb-f3e45642ccdf"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "‚úÖ Streaming DataFrame created from directory\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Streaming Query for Aggregating Total Sales per Category\n",
        "\n",
        "A streaming query is executed to compute the total purchase amount per category in real time. The results are continuously written to the console in `complete` mode, meaning the entire result is updated with each new batch of data.\n",
        "\n",
        "### Key Configurations:\n",
        "- **Grouping by \"Category\"**: Aggregates sales per product category.\n",
        "- **Summing \"Purchase Amount (USD)\"**: Computes total sales for each category.\n",
        "- **`outputMode(\"complete\")`**: Ensures the full updated result is displayed in every batch.\n",
        "- **`format(\"console\")`**: Outputs the results to the console for monitoring."
      ],
      "metadata": {
        "id": "N6WzJLPaKY6u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run a streaming query that aggregates total sales per category and writes to the console.\n",
        "stream_query = streaming_df.groupBy(\"Category\").agg({\"Purchase Amount (USD)\": \"sum\"}) \\\n",
        "    .writeStream \\\n",
        "    .outputMode(\"complete\") \\\n",
        "    .format(\"console\") \\\n",
        "    .start()\n",
        "\n",
        "print(\"‚úÖ Streaming query started. Check the console output for streaming results.\")"
      ],
      "metadata": {
        "msg_id": "eff2135f-3a36-457a-b4dc-c0390e707f84",
        "id": "AA1MbEaoKY6u"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Streaming Query Execution with Time-Controlled Execution\n",
        "\n",
        "A streaming query is initiated to compute the total purchase amount per category in real time. The query runs for a fixed duration before being stopped.\n",
        "\n",
        "### Key Configurations:\n",
        "- **Grouping by \"Category\"**: Aggregates sales data for each product category.\n",
        "- **Summing \"Purchase Amount (USD)\"**: Computes total sales in real time.\n",
        "- **`outputMode(\"complete\")`**: Displays the full updated result in every batch.\n",
        "- **`format(\"console\")`**: Prints the results to the console for monitoring.\n",
        "- **`time.sleep(20)`**: Allows the query to run for 20 seconds before stopping."
      ],
      "metadata": {
        "id": "NsI-ojnyOkJ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "# Run a streaming query to compute total sales per category in real time\n",
        "query = streaming_df.groupBy(\"Category\").agg({\"Purchase Amount (USD)\": \"sum\"}) \\\n",
        "    .writeStream \\\n",
        "    .outputMode(\"complete\") \\\n",
        "    .format(\"console\") \\\n",
        "    .start()\n",
        "\n",
        "# Let the streaming query run for a short period (e.g., 20 seconds)\n",
        "time.sleep(20)\n",
        "query.stop()\n",
        "print(\"‚úÖ Streaming query stopped.\")"
      ],
      "metadata": {
        "msg_id": "bfe3cc48-f355-4353-a879-3f3d3865a6c0",
        "id": "k0d5-ZjGKY6u",
        "outputId": "66018269-3614-47e3-8c9b-953067966f74"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "‚úÖ Streaming query stopped.\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Monitoring and Stopping the Streaming Query\n",
        "\n",
        "This section ensures the streaming query runs for a controlled duration while providing insights into its execution status before stopping it.\n",
        "\n",
        "### Key Configurations:\n",
        "- **`time.sleep(20)`**: Simulates real-time processing by allowing the query to run for 20 seconds.\n",
        "- **Query Status Monitoring**:\n",
        "  - `stream_query.status`: Retrieves the current status of the streaming query.\n",
        "  - `stream_query.lastProgress`: Fetches details about the last processing batch.\n",
        "- **Graceful Query Termination**:\n",
        "  - `stream_query.stop()`: Stops the streaming process after completion."
      ],
      "metadata": {
        "id": "K9Snir5eOq8T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "# Let the streaming query run for a while (e.g., 20 seconds) to simulate real-time processing.\n",
        "time.sleep(20)\n",
        "\n",
        "# Check streaming query status and progress (optional)\n",
        "print(\"Query Status:\", stream_query.status)\n",
        "print(\"Last Progress:\", stream_query.lastProgress)\n",
        "\n",
        "# Stop the streaming query when finished.\n",
        "stream_query.stop()\n",
        "print(\"‚úÖ Streaming query stopped.\")\n"
      ],
      "metadata": {
        "msg_id": "42278cfc-87d2-475b-9185-3f30cf3cc6fd",
        "id": "aOylUQ9IKY6u",
        "outputId": "ba199ffb-3f92-413c-93b3-0c9bfdcc63d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Query Status: {'message': 'Waiting for data to arrive', 'isDataAvailable': False, 'isTriggerActive': False}\nLast Progress: {'id': '48111222-4447-43da-97b2-20bc58538bd9', 'runId': 'aa831cd6-17c6-4c08-b38f-e39d41095744', 'name': None, 'timestamp': '2025-02-24T15:35:48.129Z', 'batchId': 0, 'numInputRows': 0, 'inputRowsPerSecond': 0.0, 'processedRowsPerSecond': 0.0, 'durationMs': {'latestOffset': 4, 'triggerExecution': 4}, 'stateOperators': [], 'sources': [{'description': 'FileStreamSource[file:/tmp/stream_data]', 'startOffset': None, 'endOffset': None, 'latestOffset': None, 'numInputRows': 0, 'inputRowsPerSecond': 0.0, 'processedRowsPerSecond': 0.0}], 'sink': {'description': 'org.apache.spark.sql.execution.streaming.ConsoleTable$@51a20257', 'numOutputRows': -1}}\n‚úÖ Streaming query stopped.\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **RealTime_Analytics.ipynb** notebook successfully demonstrates real-time data processing using **Apache Spark Structured Streaming** and **IBM Cloud Object Storage (COS)**. The implementation covers key aspects of big data analytics, including:\n",
        "\n",
        "- **Spark Session Initialization**: Configuring Spark with COS integration for seamless data access.\n",
        "- **Data Loading & Schema Definition**: Ensuring structured and type-safe data ingestion.\n",
        "- **Batch Processing**: Loading and analyzing pre-processed customer purchase data.\n",
        "- **Real-time Streaming Processing**: Implementing Structured Streaming to process continuous data streams.\n",
        "- **Aggregation & Monitoring**: Computing real-time total sales per category and tracking query progress.\n",
        "\n",
        "This notebook serves as a foundation for real-time analytics pipelines, enabling businesses to gain insights from continuously incoming data streams. Further enhancements may include **visualization dashboards, advanced machine learning models, and cloud deployment** for scalable, production-ready solutions."
      ],
      "metadata": {
        "id": "7SNmuqR4O1ie"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Q0-uPoVSO0d_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
