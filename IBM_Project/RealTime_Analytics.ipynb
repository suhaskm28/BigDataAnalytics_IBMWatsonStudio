{"metadata": {"kernelspec": {"name": "python311", "display_name": "Python 3.11 with Spark", "language": "python3"}, "language_info": {"name": "python", "version": "3.11.9", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat_minor": 4, "nbformat": 4, "cells": [{"cell_type": "code", "source": "SERVICE_NAME = \"mycos\"  # logical service name\nACCESS_KEY = \"891d47bfa17346038b970fa46d63fd41\"         # Copy from your new service credentials\nSECRET_KEY = \"25fb5c98b01a4359fc5a11fcd848ae60debd89c2ac7ebf66\"  \nENDPOINT = \"s3.eu-de.cloud-object-storage.appdomain.cloud\"  # Frankfurt region endpoint\nCOS_BUCKET = \"processed-data-bucket\"\nCOS_FILE = \"processed_customer_purchase_behavior.csv\"", "metadata": {"msg_id": "b2783017-da6b-4349-a1ae-0e94167894eb"}, "outputs": [], "execution_count": 17}, {"cell_type": "code", "source": "# Initialize Spark Session with basic Stocator config using the logical service name\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder \\\n    .appName(\"Real_Time_Analytics\") \\\n    .config(\"spark.hadoop.fs.stocator.scheme.list\", \"cos\") \\\n    .config(\"spark.hadoop.fs.cos.impl\", \"com.ibm.stocator.fs.ObjectStoreFileSystem\") \\\n    .config(f\"spark.hadoop.fs.cos.{SERVICE_NAME}.access.key\", ACCESS_KEY) \\\n    .config(f\"spark.hadoop.fs.cos.{SERVICE_NAME}.secret.key\", SECRET_KEY) \\\n    .config(f\"spark.hadoop.fs.cos.{SERVICE_NAME}.endpoint\", f\"https://{ENDPOINT}\") \\\n    .getOrCreate()\n\nprint(\"\u2705 Spark Session Initialized with COS Service Name:\", SERVICE_NAME)", "metadata": {"msg_id": "a60c7153-9a48-467b-90c6-06b05f102972"}, "outputs": [{"name": "stdout", "text": "\u2705 Spark Session Initialized with COS Service Name: mycos\n", "output_type": "stream"}], "execution_count": 18}, {"cell_type": "code", "source": "# Explicitly set the Hadoop configuration for COS credentials\nhadoopConf = spark._jsc.hadoopConfiguration()\nhadoopConf.set(f\"fs.cos.{SERVICE_NAME}.access.key\", ACCESS_KEY)\nhadoopConf.set(f\"fs.cos.{SERVICE_NAME}.secret.key\", SECRET_KEY)\nhadoopConf.set(f\"fs.cos.{SERVICE_NAME}.endpoint\", f\"https://{ENDPOINT}\")", "metadata": {"msg_id": "c6e515a9-6bde-4c73-9318-50cd767a2d55"}, "outputs": [], "execution_count": 19}, {"cell_type": "code", "source": "# Check that the configuration is set (optional)\nprint(\"Access Key from Hadoop conf:\", hadoopConf.get(f\"fs.cos.{SERVICE_NAME}.access.key\"))\nprint(\"Secret Key from Hadoop conf:\", hadoopConf.get(f\"fs.cos.{SERVICE_NAME}.secret.key\"))\nprint(\"Endpoint from Hadoop conf:\", hadoopConf.get(f\"fs.cos.{SERVICE_NAME}.endpoint\"))", "metadata": {"msg_id": "8b1eeb73-faf5-4c97-98a3-1d98fc441a15"}, "outputs": [{"name": "stdout", "text": "Access Key from Hadoop conf: 891d47bfa17346038b970fa46d63fd41\nSecret Key from Hadoop conf: 25fb5c98b01a4359fc5a11fcd848ae60debd89c2ac7ebf66\nEndpoint from Hadoop conf: https://s3.eu-de.cloud-object-storage.appdomain.cloud\n", "output_type": "stream"}], "execution_count": 20}, {"cell_type": "code", "source": "# Build the COS URL using the logical service name:\ncos_url = f\"cos://{COS_BUCKET}.{SERVICE_NAME}/{COS_FILE}\"\nprint(\"Using COS URL:\", cos_url)", "metadata": {"msg_id": "3b0d81bb-043b-48eb-9ed1-fd846c576c94"}, "outputs": [{"name": "stdout", "text": "Using COS URL: cos://processed-data-bucket.mycos/processed_customer_purchase_behavior.csv\n", "output_type": "stream"}], "execution_count": 21}, {"cell_type": "code", "source": "spark.conf.set(f\"spark.hadoop.fs.cos.{COS_BUCKET}.access.key\", ACCESS_KEY)\nspark.conf.set(f\"spark.hadoop.fs.cos.{COS_BUCKET}.secret.key\", SECRET_KEY)\nspark.conf.set(f\"spark.hadoop.fs.cos.{COS_BUCKET}.endpoint\", f\"https://{ENDPOINT}\")\n\nprint(\"\u2705 Spark Configuration Updated with COS Credentials\")\n", "metadata": {"msg_id": "e69975ad-6e82-4ff1-995e-714251ac1cc4"}, "outputs": [{"name": "stdout", "text": "\u2705 Spark Configuration Updated with COS Credentials\n", "output_type": "stream"}], "execution_count": 22}, {"cell_type": "code", "source": "# Define the schema for your dataset\nfrom pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n\nschema = StructType([\n    StructField(\"Customer ID\", StringType(), True),\n    StructField(\"Age\", IntegerType(), True),\n    StructField(\"Gender\", StringType(), True),\n    StructField(\"Item Purchased\", StringType(), True),\n    StructField(\"Category\", StringType(), True),\n    StructField(\"Purchase Amount (USD)\", DoubleType(), True),\n    StructField(\"Location\", StringType(), True),\n    StructField(\"Season\", StringType(), True),\n    StructField(\"High Value Customer\", StringType(), True),\n])\n\n# Read the data from COS using Stocator\nprocessed_df = spark.read \\\n    .option(\"header\", \"true\") \\\n    .schema(schema) \\\n    .csv(cos_url)\n\nprint(\"\u2705 Processed Data Loaded Successfully!\")\nprocessed_df.show(5)", "metadata": {"msg_id": "8043e3a3-a4a3-4eb4-b707-0542dd37d787"}, "outputs": [{"name": "stdout", "text": "\u2705 Processed Data Loaded Successfully!\n+-----------+---+------+--------------+--------+---------------------+-------------+------+-------------------+\n|Customer ID|Age|Gender|Item Purchased|Category|Purchase Amount (USD)|     Location|Season|High Value Customer|\n+-----------+---+------+--------------+--------+---------------------+-------------+------+-------------------+\n|          1| 55|  Male|        Blouse|Clothing|                 53.0|     Kentucky|     L|               Gray|\n|          2| 19|  Male|       Sweater|Clothing|                 64.0|        Maine|     L|             Maroon|\n|          3| 50|  Male|         Jeans|Clothing|                 73.0|Massachusetts|     S|             Maroon|\n|          4| 21|  Male|       Sandals|Footwear|                 90.0| Rhode Island|     M|             Maroon|\n|          5| 45|  Male|        Blouse|Clothing|                 49.0|       Oregon|     M|          Turquoise|\n+-----------+---+------+--------------+--------+---------------------+-------------+------+-------------------+\nonly showing top 5 rows\n\n", "output_type": "stream"}], "execution_count": 23}, {"cell_type": "markdown", "source": "## Write Batch Data to a Temporary Directory", "metadata": {}}, {"cell_type": "code", "source": "import os\n\n# Define a temporary directory to store CSV files (adjust path if necessary)\ntemp_dir = \"/tmp/stream_data\"\nos.makedirs(temp_dir, exist_ok=True)\n\n# Write the processed DataFrame to this directory as CSV files (overwrite existing)\nprocessed_df.write.mode(\"overwrite\").option(\"header\", \"true\").csv(temp_dir)\nprint(\"\u2705 Processed data written to:\", temp_dir)\n", "metadata": {"msg_id": "1d194573-8a2e-49eb-9e87-b9a229313934"}, "outputs": [{"name": "stdout", "text": "\u2705 Processed data written to: /tmp/stream_data\n", "output_type": "stream"}], "execution_count": 25}, {"cell_type": "markdown", "source": "## Read the Directory as a Streaming DataFrame", "metadata": {}}, {"cell_type": "code", "source": "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n\n# Use the same schema as before\nschema = StructType([\n    StructField(\"Customer ID\", StringType(), True),\n    StructField(\"Age\", IntegerType(), True),\n    StructField(\"Gender\", StringType(), True),\n    StructField(\"Item Purchased\", StringType(), True),\n    StructField(\"Category\", StringType(), True),\n    StructField(\"Purchase Amount (USD)\", DoubleType(), True),\n    StructField(\"Location\", StringType(), True),\n    StructField(\"Season\", StringType(), True),\n    StructField(\"High Value Customer\", StringType(), True),\n])\n\n# Read streaming data from the temporary directory\nstreaming_df = spark.readStream \\\n    .option(\"header\", \"true\") \\\n    .option(\"maxFilesPerTrigger\", 1) \\\n    .schema(schema) \\\n    .csv(temp_dir)\n\nprint(\"\u2705 Streaming DataFrame created from directory\")\n", "metadata": {"msg_id": "fba84301-195c-4d96-8cb5-1349474baecc"}, "outputs": [{"name": "stdout", "text": "\u2705 Streaming DataFrame created from directory\n", "output_type": "stream"}], "execution_count": 26}, {"cell_type": "markdown", "source": "## Run a Real-Time Aggregation on the Streaming DataFrame", "metadata": {}}, {"cell_type": "code", "source": "# Run a streaming query that aggregates total sales per category and writes to the console.\nstream_query = streaming_df.groupBy(\"Category\").agg({\"Purchase Amount (USD)\": \"sum\"}) \\\n    .writeStream \\\n    .outputMode(\"complete\") \\\n    .format(\"console\") \\\n    .start()\n\nprint(\"\u2705 Streaming query started. Check the console output for streaming results.\")", "metadata": {"msg_id": "eff2135f-3a36-457a-b4dc-c0390e707f84"}, "outputs": [], "execution_count": 34}, {"cell_type": "code", "source": "import time\n\n# Run a streaming query to compute total sales per category in real time\nquery = streaming_df.groupBy(\"Category\").agg({\"Purchase Amount (USD)\": \"sum\"}) \\\n    .writeStream \\\n    .outputMode(\"complete\") \\\n    .format(\"console\") \\\n    .start()\n\n# Let the streaming query run for a short period (e.g., 20 seconds)\ntime.sleep(20)\nquery.stop()\nprint(\"\u2705 Streaming query stopped.\")", "metadata": {"msg_id": "bfe3cc48-f355-4353-a879-3f3d3865a6c0"}, "outputs": [{"name": "stdout", "text": "\u2705 Streaming query stopped.\n", "output_type": "stream"}], "execution_count": 32}, {"cell_type": "code", "source": "import time\n\n# Let the streaming query run for a while (e.g., 20 seconds) to simulate real-time processing.\ntime.sleep(20)\n\n# Check streaming query status and progress (optional)\nprint(\"Query Status:\", stream_query.status)\nprint(\"Last Progress:\", stream_query.lastProgress)\n\n# Stop the streaming query when finished.\nstream_query.stop()\nprint(\"\u2705 Streaming query stopped.\")\n", "metadata": {"msg_id": "42278cfc-87d2-475b-9185-3f30cf3cc6fd"}, "outputs": [{"name": "stdout", "text": "Query Status: {'message': 'Waiting for data to arrive', 'isDataAvailable': False, 'isTriggerActive': False}\nLast Progress: {'id': '48111222-4447-43da-97b2-20bc58538bd9', 'runId': 'aa831cd6-17c6-4c08-b38f-e39d41095744', 'name': None, 'timestamp': '2025-02-24T15:35:48.129Z', 'batchId': 0, 'numInputRows': 0, 'inputRowsPerSecond': 0.0, 'processedRowsPerSecond': 0.0, 'durationMs': {'latestOffset': 4, 'triggerExecution': 4}, 'stateOperators': [], 'sources': [{'description': 'FileStreamSource[file:/tmp/stream_data]', 'startOffset': None, 'endOffset': None, 'latestOffset': None, 'numInputRows': 0, 'inputRowsPerSecond': 0.0, 'processedRowsPerSecond': 0.0}], 'sink': {'description': 'org.apache.spark.sql.execution.streaming.ConsoleTable$@51a20257', 'numOutputRows': -1}}\n\u2705 Streaming query stopped.\n", "output_type": "stream"}], "execution_count": 33}, {"cell_type": "code", "source": "", "metadata": {}, "outputs": [], "execution_count": null}]}