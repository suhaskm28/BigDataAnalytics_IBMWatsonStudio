{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "msg_id": "7008c41b-0c12-48bb-943d-f20268fa09cb"
   },
   "outputs": [],
   "source": [
    "# !pip install pyspark dash dash-core-components dash-html-components plotly pandas numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "msg_id": "148d4028-7560-41b1-a055-79503507a5e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Spark Session Initialized!\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Predictive_Modeling\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"âœ… Spark Session Initialized!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_df = spark.createDataFrame([(1, \"test\")], [\"id\", \"value\"])\n",
    "# test_df.write.mode(\"overwrite\").parquet(\"file:///C:/tmp/test_parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# print(sys.version)\n",
    "# import pyspark\n",
    "# print(pyspark.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Processed Data Loaded Successfully!\n",
      "root\n",
      " |-- Customer ID: string (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Gender: string (nullable = true)\n",
      " |-- Item Purchased: string (nullable = true)\n",
      " |-- Category: string (nullable = true)\n",
      " |-- Purchase Amount (USD): double (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- Size: string (nullable = true)\n",
      " |-- Color: string (nullable = true)\n",
      " |-- Season: string (nullable = true)\n",
      " |-- Review Rating: double (nullable = true)\n",
      " |-- Subscription Status: integer (nullable = true)\n",
      " |-- Payment Method: string (nullable = true)\n",
      " |-- Shipping Type: string (nullable = true)\n",
      " |-- Discount Applied: integer (nullable = true)\n",
      " |-- Promo Code Used: integer (nullable = true)\n",
      " |-- Previous Purchases: integer (nullable = true)\n",
      " |-- Preferred Payment Method: string (nullable = true)\n",
      " |-- Frequency of Purchases: string (nullable = true)\n",
      " |-- High Value Customer: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "\n",
    "# Define your file path (Ensure this is the correct path)\n",
    "LOCAL_FILE_PATH = \"processed_data_bucket/processed_customer_purchase_behavior.csv\"\n",
    "\n",
    "# Define the schema of your processed data\n",
    "schema = StructType([\n",
    "    StructField(\"Customer ID\", StringType(), True),\n",
    "    StructField(\"Age\", IntegerType(), True),\n",
    "    StructField(\"Gender\", StringType(), True),\n",
    "    StructField(\"Item Purchased\", StringType(), True),\n",
    "    StructField(\"Category\", StringType(), True),\n",
    "    StructField(\"Purchase Amount (USD)\", DoubleType(), True),\n",
    "    StructField(\"Location\", StringType(), True),\n",
    "    StructField(\"Size\", StringType(), True),\n",
    "    StructField(\"Color\", StringType(), True),\n",
    "    StructField(\"Season\", StringType(), True),\n",
    "    StructField(\"Review Rating\", DoubleType(), True),\n",
    "    StructField(\"Subscription Status\", IntegerType(), True),  \n",
    "    StructField(\"Payment Method\", StringType(), True),\n",
    "    StructField(\"Shipping Type\", StringType(), True),\n",
    "    StructField(\"Discount Applied\", IntegerType(), True),\n",
    "    StructField(\"Promo Code Used\", IntegerType(), True), \n",
    "    StructField(\"Previous Purchases\", IntegerType(), True),  \n",
    "    StructField(\"Preferred Payment Method\", StringType(), True),\n",
    "    StructField(\"Frequency of Purchases\", StringType(), True),\n",
    "    StructField(\"High Value Customer\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Read the processed data from local CSV file\n",
    "processed_df = spark.read.option(\"header\", \"true\").schema(schema).csv(LOCAL_FILE_PATH)\n",
    "\n",
    "# Display schema and first few rows\n",
    "print(\"âœ… Processed Data Loaded Successfully!\")\n",
    "processed_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.3\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "print(pd.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Customer ID</th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Item Purchased</th>\n",
       "      <th>Category</th>\n",
       "      <th>Purchase Amount (USD)</th>\n",
       "      <th>Location</th>\n",
       "      <th>Size</th>\n",
       "      <th>Color</th>\n",
       "      <th>Season</th>\n",
       "      <th>Review Rating</th>\n",
       "      <th>Subscription Status</th>\n",
       "      <th>Payment Method</th>\n",
       "      <th>Shipping Type</th>\n",
       "      <th>Discount Applied</th>\n",
       "      <th>Promo Code Used</th>\n",
       "      <th>Previous Purchases</th>\n",
       "      <th>Preferred Payment Method</th>\n",
       "      <th>Frequency of Purchases</th>\n",
       "      <th>High Value Customer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>55</td>\n",
       "      <td>Male</td>\n",
       "      <td>Blouse</td>\n",
       "      <td>Clothing</td>\n",
       "      <td>53.0</td>\n",
       "      <td>Kentucky</td>\n",
       "      <td>L</td>\n",
       "      <td>Gray</td>\n",
       "      <td>Winter</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1</td>\n",
       "      <td>Credit Card</td>\n",
       "      <td>Express</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>Venmo</td>\n",
       "      <td>Fortnightly</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>19</td>\n",
       "      <td>Male</td>\n",
       "      <td>Sweater</td>\n",
       "      <td>Clothing</td>\n",
       "      <td>64.0</td>\n",
       "      <td>Maine</td>\n",
       "      <td>L</td>\n",
       "      <td>Maroon</td>\n",
       "      <td>Winter</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1</td>\n",
       "      <td>Bank Transfer</td>\n",
       "      <td>Express</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Cash</td>\n",
       "      <td>Fortnightly</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>Male</td>\n",
       "      <td>Jeans</td>\n",
       "      <td>Clothing</td>\n",
       "      <td>73.0</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>S</td>\n",
       "      <td>Maroon</td>\n",
       "      <td>Spring</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cash</td>\n",
       "      <td>Free Shipping</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>Credit Card</td>\n",
       "      <td>Weekly</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>21</td>\n",
       "      <td>Male</td>\n",
       "      <td>Sandals</td>\n",
       "      <td>Footwear</td>\n",
       "      <td>90.0</td>\n",
       "      <td>Rhode Island</td>\n",
       "      <td>M</td>\n",
       "      <td>Maroon</td>\n",
       "      <td>Spring</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1</td>\n",
       "      <td>PayPal</td>\n",
       "      <td>Next Day Air</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>49</td>\n",
       "      <td>PayPal</td>\n",
       "      <td>Weekly</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>45</td>\n",
       "      <td>Male</td>\n",
       "      <td>Blouse</td>\n",
       "      <td>Clothing</td>\n",
       "      <td>49.0</td>\n",
       "      <td>Oregon</td>\n",
       "      <td>M</td>\n",
       "      <td>Turquoise</td>\n",
       "      <td>Spring</td>\n",
       "      <td>2.7</td>\n",
       "      <td>1</td>\n",
       "      <td>Cash</td>\n",
       "      <td>Free Shipping</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>31</td>\n",
       "      <td>PayPal</td>\n",
       "      <td>Annually</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>46</td>\n",
       "      <td>Male</td>\n",
       "      <td>Sneakers</td>\n",
       "      <td>Footwear</td>\n",
       "      <td>20.0</td>\n",
       "      <td>Wyoming</td>\n",
       "      <td>M</td>\n",
       "      <td>White</td>\n",
       "      <td>Summer</td>\n",
       "      <td>2.9</td>\n",
       "      <td>1</td>\n",
       "      <td>Venmo</td>\n",
       "      <td>Standard</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>Venmo</td>\n",
       "      <td>Weekly</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>63</td>\n",
       "      <td>Male</td>\n",
       "      <td>Shirt</td>\n",
       "      <td>Clothing</td>\n",
       "      <td>85.0</td>\n",
       "      <td>Montana</td>\n",
       "      <td>M</td>\n",
       "      <td>Gray</td>\n",
       "      <td>Fall</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1</td>\n",
       "      <td>Debit Card</td>\n",
       "      <td>Free Shipping</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>49</td>\n",
       "      <td>Cash</td>\n",
       "      <td>Quarterly</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>27</td>\n",
       "      <td>Male</td>\n",
       "      <td>Shorts</td>\n",
       "      <td>Clothing</td>\n",
       "      <td>34.0</td>\n",
       "      <td>Louisiana</td>\n",
       "      <td>L</td>\n",
       "      <td>Charcoal</td>\n",
       "      <td>Winter</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1</td>\n",
       "      <td>Debit Card</td>\n",
       "      <td>Free Shipping</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>Credit Card</td>\n",
       "      <td>Weekly</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>26</td>\n",
       "      <td>Male</td>\n",
       "      <td>Coat</td>\n",
       "      <td>Outerwear</td>\n",
       "      <td>97.0</td>\n",
       "      <td>West Virginia</td>\n",
       "      <td>L</td>\n",
       "      <td>Silver</td>\n",
       "      <td>Summer</td>\n",
       "      <td>2.6</td>\n",
       "      <td>1</td>\n",
       "      <td>Venmo</td>\n",
       "      <td>Express</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>Venmo</td>\n",
       "      <td>Annually</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>57</td>\n",
       "      <td>Male</td>\n",
       "      <td>Handbag</td>\n",
       "      <td>Accessories</td>\n",
       "      <td>31.0</td>\n",
       "      <td>Missouri</td>\n",
       "      <td>M</td>\n",
       "      <td>Pink</td>\n",
       "      <td>Spring</td>\n",
       "      <td>4.8</td>\n",
       "      <td>1</td>\n",
       "      <td>PayPal</td>\n",
       "      <td>2-Day Shipping</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>Cash</td>\n",
       "      <td>Quarterly</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Customer ID  Age Gender Item Purchased     Category  Purchase Amount (USD)  \\\n",
       "0           1   55   Male         Blouse     Clothing                   53.0   \n",
       "1           2   19   Male        Sweater     Clothing                   64.0   \n",
       "2           3   50   Male          Jeans     Clothing                   73.0   \n",
       "3           4   21   Male        Sandals     Footwear                   90.0   \n",
       "4           5   45   Male         Blouse     Clothing                   49.0   \n",
       "5           6   46   Male       Sneakers     Footwear                   20.0   \n",
       "6           7   63   Male          Shirt     Clothing                   85.0   \n",
       "7           8   27   Male         Shorts     Clothing                   34.0   \n",
       "8           9   26   Male           Coat    Outerwear                   97.0   \n",
       "9          10   57   Male        Handbag  Accessories                   31.0   \n",
       "\n",
       "        Location Size      Color  Season  Review Rating  Subscription Status  \\\n",
       "0       Kentucky    L       Gray  Winter            3.1                    1   \n",
       "1          Maine    L     Maroon  Winter            3.1                    1   \n",
       "2  Massachusetts    S     Maroon  Spring            3.1                    1   \n",
       "3   Rhode Island    M     Maroon  Spring            3.5                    1   \n",
       "4         Oregon    M  Turquoise  Spring            2.7                    1   \n",
       "5        Wyoming    M      White  Summer            2.9                    1   \n",
       "6        Montana    M       Gray    Fall            3.2                    1   \n",
       "7      Louisiana    L   Charcoal  Winter            3.2                    1   \n",
       "8  West Virginia    L     Silver  Summer            2.6                    1   \n",
       "9       Missouri    M       Pink  Spring            4.8                    1   \n",
       "\n",
       "  Payment Method   Shipping Type  Discount Applied  Promo Code Used  \\\n",
       "0    Credit Card         Express                 1                1   \n",
       "1  Bank Transfer         Express                 1                1   \n",
       "2           Cash   Free Shipping                 1                1   \n",
       "3         PayPal    Next Day Air                 1                1   \n",
       "4           Cash   Free Shipping                 1                1   \n",
       "5          Venmo        Standard                 1                1   \n",
       "6     Debit Card   Free Shipping                 1                1   \n",
       "7     Debit Card   Free Shipping                 1                1   \n",
       "8          Venmo         Express                 1                1   \n",
       "9         PayPal  2-Day Shipping                 1                1   \n",
       "\n",
       "   Previous Purchases Preferred Payment Method Frequency of Purchases  \\\n",
       "0                  14                    Venmo            Fortnightly   \n",
       "1                   2                     Cash            Fortnightly   \n",
       "2                  23              Credit Card                 Weekly   \n",
       "3                  49                   PayPal                 Weekly   \n",
       "4                  31                   PayPal               Annually   \n",
       "5                  14                    Venmo                 Weekly   \n",
       "6                  49                     Cash              Quarterly   \n",
       "7                  19              Credit Card                 Weekly   \n",
       "8                   8                    Venmo               Annually   \n",
       "9                   4                     Cash              Quarterly   \n",
       "\n",
       "  High Value Customer  \n",
       "0                  No  \n",
       "1                  No  \n",
       "2                  No  \n",
       "3                 Yes  \n",
       "4                  No  \n",
       "5                  No  \n",
       "6                 Yes  \n",
       "7                  No  \n",
       "8                 Yes  \n",
       "9                  No  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_df.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Missing Values per Column:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Customer ID</th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Item Purchased</th>\n",
       "      <th>Category</th>\n",
       "      <th>Purchase Amount (USD)</th>\n",
       "      <th>Location</th>\n",
       "      <th>Size</th>\n",
       "      <th>Color</th>\n",
       "      <th>Season</th>\n",
       "      <th>Review Rating</th>\n",
       "      <th>Subscription Status</th>\n",
       "      <th>Payment Method</th>\n",
       "      <th>Shipping Type</th>\n",
       "      <th>Discount Applied</th>\n",
       "      <th>Promo Code Used</th>\n",
       "      <th>Previous Purchases</th>\n",
       "      <th>Preferred Payment Method</th>\n",
       "      <th>Frequency of Purchases</th>\n",
       "      <th>High Value Customer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Customer ID  Age  Gender  Item Purchased  Category  Purchase Amount (USD)  \\\n",
       "0            0    0       0               0         0                      0   \n",
       "\n",
       "   Location  Size  Color  Season  Review Rating  Subscription Status  \\\n",
       "0         0     0      0       0              0                    0   \n",
       "\n",
       "   Payment Method  Shipping Type  Discount Applied  Promo Code Used  \\\n",
       "0               0              0                 0                0   \n",
       "\n",
       "   Previous Purchases  Preferred Payment Method  Frequency of Purchases  \\\n",
       "0                   0                         0                       0   \n",
       "\n",
       "   High Value Customer  \n",
       "0                    0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, sum\n",
    "\n",
    "# Count missing values in each column\n",
    "missing_values = processed_df.select([sum(col(c).isNull().cast(\"int\")).alias(c) for c in processed_df.columns])\n",
    "print(\"âœ… Missing Values per Column:\")\n",
    "missing_values.limit(1).toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "msg_id": "3566b47f-bdc3-42e1-943c-7a385c69ec36"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Unique Value Counts per Column:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Customer ID</th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Item Purchased</th>\n",
       "      <th>Category</th>\n",
       "      <th>Purchase Amount (USD)</th>\n",
       "      <th>Location</th>\n",
       "      <th>Size</th>\n",
       "      <th>Color</th>\n",
       "      <th>Season</th>\n",
       "      <th>Review Rating</th>\n",
       "      <th>Subscription Status</th>\n",
       "      <th>Payment Method</th>\n",
       "      <th>Shipping Type</th>\n",
       "      <th>Discount Applied</th>\n",
       "      <th>Promo Code Used</th>\n",
       "      <th>Previous Purchases</th>\n",
       "      <th>Preferred Payment Method</th>\n",
       "      <th>Frequency of Purchases</th>\n",
       "      <th>High Value Customer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3900</td>\n",
       "      <td>53</td>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "      <td>4</td>\n",
       "      <td>81</td>\n",
       "      <td>50</td>\n",
       "      <td>4</td>\n",
       "      <td>25</td>\n",
       "      <td>4</td>\n",
       "      <td>26</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Customer ID  Age  Gender  Item Purchased  Category  Purchase Amount (USD)  \\\n",
       "0         3900   53       2              25         4                     81   \n",
       "\n",
       "   Location  Size  Color  Season  Review Rating  Subscription Status  \\\n",
       "0        50     4     25       4             26                    2   \n",
       "\n",
       "   Payment Method  Shipping Type  Discount Applied  Promo Code Used  \\\n",
       "0               6              6                 2                2   \n",
       "\n",
       "   Previous Purchases  Preferred Payment Method  Frequency of Purchases  \\\n",
       "0                  50                         6                       7   \n",
       "\n",
       "   High Value Customer  \n",
       "0                    2  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import countDistinct\n",
    "\n",
    "# Count unique values for each column\n",
    "unique_counts = processed_df.select([countDistinct(col(c)).alias(c) for c in processed_df.columns])\n",
    "\n",
    "print(\"âœ… Unique Value Counts per Column:\")\n",
    "unique_counts.limit(1).toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "msg_id": "24592251-a9a8-4d3d-8ee9-897ae413abc7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Distribution for Subscription Status:\n",
      "+-------------------+-----+\n",
      "|Subscription Status|count|\n",
      "+-------------------+-----+\n",
      "|                  1| 1053|\n",
      "|                  0| 2847|\n",
      "+-------------------+-----+\n",
      "\n",
      "âœ… Distribution for Discount Applied:\n",
      "+----------------+-----+\n",
      "|Discount Applied|count|\n",
      "+----------------+-----+\n",
      "|               1| 1677|\n",
      "|               0| 2223|\n",
      "+----------------+-----+\n",
      "\n",
      "âœ… Distribution for Promo Code Used:\n",
      "+---------------+-----+\n",
      "|Promo Code Used|count|\n",
      "+---------------+-----+\n",
      "|              1| 1677|\n",
      "|              0| 2223|\n",
      "+---------------+-----+\n",
      "\n",
      "âœ… Distribution for High Value Customer:\n",
      "+-------------------+-----+\n",
      "|High Value Customer|count|\n",
      "+-------------------+-----+\n",
      "|                 No| 2974|\n",
      "|                Yes|  926|\n",
      "+-------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "categorical_features = [\"Subscription Status\", \"Discount Applied\", \"Promo Code Used\", \"High Value Customer\"]\n",
    "\n",
    "for col_name in categorical_features:\n",
    "    print(f\"âœ… Distribution for {col_name}:\")\n",
    "    processed_df.groupBy(col_name).count().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "msg_id": "594c5149-af16-44cf-84e7-c2682ec6d328"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Purchase Amount Statistics:\n",
      "+------------+------------+-----------------+---------------+\n",
      "|Min_Purchase|Max_Purchase|    Mean_Purchase|Median_Purchase|\n",
      "+------------+------------+-----------------+---------------+\n",
      "|        20.0|       100.0|59.76435897435898|           60.0|\n",
      "+------------+------------+-----------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import min, max, avg, percentile_approx\n",
    "\n",
    "purchase_stats = processed_df.select(\n",
    "    min(\"Purchase Amount (USD)\").alias(\"Min_Purchase\"),\n",
    "    max(\"Purchase Amount (USD)\").alias(\"Max_Purchase\"),\n",
    "    avg(\"Purchase Amount (USD)\").alias(\"Mean_Purchase\"),\n",
    "    percentile_approx(\"Purchase Amount (USD)\", 0.5).alias(\"Median_Purchase\")\n",
    ")\n",
    "\n",
    "print(\"âœ… Purchase Amount Statistics:\")\n",
    "purchase_stats.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "msg_id": "cfa36b54-ac43-4df9-bb2e-e3538ee8adee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Number of Outliers in 'Purchase Amount (USD)': 0\n"
     ]
    }
   ],
   "source": [
    "quantiles = processed_df.approxQuantile(\"Purchase Amount (USD)\", [0.25, 0.75], 0.05)\n",
    "Q1, Q3 = quantiles[0], quantiles[1]\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "outliers = processed_df.filter(\n",
    "    (col(\"Purchase Amount (USD)\") < lower_bound) | (col(\"Purchase Amount (USD)\") > upper_bound)\n",
    ").count()\n",
    "\n",
    "print(f\"âœ… Number of Outliers in 'Purchase Amount (USD)': {outliers}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Distribution for Gender:\n",
      "+------+-----+\n",
      "|Gender|count|\n",
      "+------+-----+\n",
      "|Female|1248 |\n",
      "|Male  |2652 |\n",
      "+------+-----+\n",
      "\n",
      "âœ… Distribution for Item Purchased:\n",
      "+--------------+-----+\n",
      "|Item Purchased|count|\n",
      "+--------------+-----+\n",
      "|T-shirt       |147  |\n",
      "|Jacket        |163  |\n",
      "|Sneakers      |145  |\n",
      "|Belt          |161  |\n",
      "|Dress         |166  |\n",
      "|Sweater       |164  |\n",
      "|Hat           |154  |\n",
      "|Coat          |161  |\n",
      "|Sunglasses    |161  |\n",
      "|Pants         |171  |\n",
      "|Hoodie        |151  |\n",
      "|Handbag       |153  |\n",
      "|Gloves        |140  |\n",
      "|Backpack      |143  |\n",
      "|Shirt         |169  |\n",
      "|Shoes         |150  |\n",
      "|Blouse        |171  |\n",
      "|Jewelry       |171  |\n",
      "|Boots         |144  |\n",
      "|Shorts        |157  |\n",
      "+--------------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "âœ… Distribution for Category:\n",
      "+-----------+-----+\n",
      "|Category   |count|\n",
      "+-----------+-----+\n",
      "|Outerwear  |324  |\n",
      "|Clothing   |1737 |\n",
      "|Footwear   |599  |\n",
      "|Accessories|1240 |\n",
      "+-----------+-----+\n",
      "\n",
      "âœ… Distribution for Location:\n",
      "+-------------+-----+\n",
      "|Location     |count|\n",
      "+-------------+-----+\n",
      "|Utah         |71   |\n",
      "|Hawaii       |65   |\n",
      "|Minnesota    |88   |\n",
      "|Ohio         |77   |\n",
      "|Oregon       |74   |\n",
      "|Arkansas     |79   |\n",
      "|Texas        |77   |\n",
      "|North Dakota |83   |\n",
      "|Pennsylvania |74   |\n",
      "|Connecticut  |78   |\n",
      "|Vermont      |85   |\n",
      "|Nebraska     |87   |\n",
      "|Nevada       |87   |\n",
      "|Washington   |73   |\n",
      "|Illinois     |92   |\n",
      "|Oklahoma     |75   |\n",
      "|Delaware     |86   |\n",
      "|Alaska       |72   |\n",
      "|New Mexico   |81   |\n",
      "|West Virginia|81   |\n",
      "+-------------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "âœ… Distribution for Size:\n",
      "+----+-----+\n",
      "|Size|count|\n",
      "+----+-----+\n",
      "|XL  |429  |\n",
      "|L   |1053 |\n",
      "|M   |1755 |\n",
      "|S   |663  |\n",
      "+----+-----+\n",
      "\n",
      "âœ… Distribution for Color:\n",
      "+---------+-----+\n",
      "|Color    |count|\n",
      "+---------+-----+\n",
      "|Teal     |172  |\n",
      "|Olive    |177  |\n",
      "|Orange   |154  |\n",
      "|Peach    |149  |\n",
      "|Indigo   |147  |\n",
      "|Cyan     |166  |\n",
      "|Beige    |147  |\n",
      "|Turquoise|145  |\n",
      "|Silver   |173  |\n",
      "|Green    |169  |\n",
      "|Lavender |147  |\n",
      "|Purple   |151  |\n",
      "|Gray     |159  |\n",
      "|Blue     |152  |\n",
      "|White    |142  |\n",
      "|Brown    |141  |\n",
      "|Gold     |138  |\n",
      "|Magenta  |152  |\n",
      "|Charcoal |153  |\n",
      "|Violet   |166  |\n",
      "+---------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "âœ… Distribution for Season:\n",
      "+------+-----+\n",
      "|Season|count|\n",
      "+------+-----+\n",
      "|Spring|999  |\n",
      "|Summer|955  |\n",
      "|Fall  |975  |\n",
      "|Winter|971  |\n",
      "+------+-----+\n",
      "\n",
      "âœ… Distribution for Payment Method:\n",
      "+--------------+-----+\n",
      "|Payment Method|count|\n",
      "+--------------+-----+\n",
      "|Credit Card   |696  |\n",
      "|Bank Transfer |632  |\n",
      "|PayPal        |638  |\n",
      "|Cash          |648  |\n",
      "|Debit Card    |633  |\n",
      "|Venmo         |653  |\n",
      "+--------------+-----+\n",
      "\n",
      "âœ… Distribution for Shipping Type:\n",
      "+--------------+-----+\n",
      "|Shipping Type |count|\n",
      "+--------------+-----+\n",
      "|Free Shipping |675  |\n",
      "|Next Day Air  |648  |\n",
      "|Express       |646  |\n",
      "|Store Pickup  |650  |\n",
      "|2-Day Shipping|627  |\n",
      "|Standard      |654  |\n",
      "+--------------+-----+\n",
      "\n",
      "âœ… Distribution for Preferred Payment Method:\n",
      "+------------------------+-----+\n",
      "|Preferred Payment Method|count|\n",
      "+------------------------+-----+\n",
      "|Credit Card             |671  |\n",
      "|Bank Transfer           |612  |\n",
      "|PayPal                  |677  |\n",
      "|Cash                    |670  |\n",
      "|Debit Card              |636  |\n",
      "|Venmo                   |634  |\n",
      "+------------------------+-----+\n",
      "\n",
      "âœ… Distribution for Frequency of Purchases:\n",
      "+----------------------+-----+\n",
      "|Frequency of Purchases|count|\n",
      "+----------------------+-----+\n",
      "|Quarterly             |563  |\n",
      "|Bi-Weekly             |547  |\n",
      "|Annually              |572  |\n",
      "|Every 3 Months        |584  |\n",
      "|Monthly               |553  |\n",
      "|Fortnightly           |542  |\n",
      "|Weekly                |539  |\n",
      "+----------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count\n",
    "\n",
    "# Categorical columns to analyze\n",
    "categorical_cols = [\"Gender\", \"Item Purchased\", \"Category\", \"Location\", \"Size\", \"Color\", \n",
    "                    \"Season\", \"Payment Method\", \"Shipping Type\", \"Preferred Payment Method\", \"Frequency of Purchases\"]\n",
    "\n",
    "for col_name in categorical_cols:\n",
    "    print(f\"âœ… Distribution for {col_name}:\")\n",
    "    processed_df.groupBy(col_name).agg(count(\"*\").alias(\"count\")).show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Categorical Encoding Completed!\n",
      "root\n",
      " |-- Customer ID: string (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Gender: string (nullable = true)\n",
      " |-- Purchase Amount (USD): double (nullable = true)\n",
      " |-- Review Rating: double (nullable = true)\n",
      " |-- Subscription Status: integer (nullable = true)\n",
      " |-- Discount Applied: integer (nullable = true)\n",
      " |-- Promo Code Used: integer (nullable = true)\n",
      " |-- Previous Purchases: integer (nullable = true)\n",
      " |-- High Value Customer: string (nullable = true)\n",
      " |-- Item Purchased_index: double (nullable = false)\n",
      " |-- Category_index: double (nullable = false)\n",
      " |-- Location_index: double (nullable = false)\n",
      " |-- Size_index: double (nullable = false)\n",
      " |-- Color_index: double (nullable = false)\n",
      " |-- Season_index: double (nullable = false)\n",
      " |-- Payment Method_index: double (nullable = false)\n",
      " |-- Shipping Type_index: double (nullable = false)\n",
      " |-- Preferred Payment Method_index: double (nullable = false)\n",
      " |-- Frequency of Purchases_index: double (nullable = false)\n",
      " |-- Item Purchased_encoded: vector (nullable = true)\n",
      " |-- Category_encoded: vector (nullable = true)\n",
      " |-- Location_encoded: vector (nullable = true)\n",
      " |-- Size_encoded: vector (nullable = true)\n",
      " |-- Color_encoded: vector (nullable = true)\n",
      " |-- Season_encoded: vector (nullable = true)\n",
      " |-- Payment Method_encoded: vector (nullable = true)\n",
      " |-- Shipping Type_encoded: vector (nullable = true)\n",
      " |-- Preferred Payment Method_encoded: vector (nullable = true)\n",
      " |-- Frequency of Purchases_encoded: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "categorical_features = [\"Item Purchased\", \"Category\", \"Location\", \"Size\", \"Color\", \n",
    "                        \"Season\", \"Payment Method\", \"Shipping Type\", \n",
    "                        \"Preferred Payment Method\", \"Frequency of Purchases\"]\n",
    "\n",
    "indexers = [StringIndexer(inputCol=col, outputCol=col+\"_index\") for col in categorical_features]\n",
    "encoders = [OneHotEncoder(inputCol=col+\"_index\", outputCol=col+\"_encoded\") for col in categorical_features]\n",
    "\n",
    "pipeline = Pipeline(stages=indexers + encoders)\n",
    "processed_df = pipeline.fit(processed_df).transform(processed_df)\n",
    "\n",
    "# Drop original categorical columns after encoding\n",
    "processed_df = processed_df.drop(*categorical_features)\n",
    "\n",
    "print(\"âœ… Categorical Encoding Completed!\")\n",
    "processed_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Numeric Feature Scaling Completed!\n",
      "root\n",
      " |-- Customer ID: string (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Gender: string (nullable = true)\n",
      " |-- Subscription Status: integer (nullable = true)\n",
      " |-- Discount Applied: integer (nullable = true)\n",
      " |-- Promo Code Used: integer (nullable = true)\n",
      " |-- Previous Purchases: integer (nullable = true)\n",
      " |-- High Value Customer: string (nullable = true)\n",
      " |-- Item Purchased_index: double (nullable = false)\n",
      " |-- Category_index: double (nullable = false)\n",
      " |-- Location_index: double (nullable = false)\n",
      " |-- Size_index: double (nullable = false)\n",
      " |-- Color_index: double (nullable = false)\n",
      " |-- Season_index: double (nullable = false)\n",
      " |-- Payment Method_index: double (nullable = false)\n",
      " |-- Shipping Type_index: double (nullable = false)\n",
      " |-- Preferred Payment Method_index: double (nullable = false)\n",
      " |-- Frequency of Purchases_index: double (nullable = false)\n",
      " |-- Item Purchased_encoded: vector (nullable = true)\n",
      " |-- Category_encoded: vector (nullable = true)\n",
      " |-- Location_encoded: vector (nullable = true)\n",
      " |-- Size_encoded: vector (nullable = true)\n",
      " |-- Color_encoded: vector (nullable = true)\n",
      " |-- Season_encoded: vector (nullable = true)\n",
      " |-- Payment Method_encoded: vector (nullable = true)\n",
      " |-- Shipping Type_encoded: vector (nullable = true)\n",
      " |-- Preferred Payment Method_encoded: vector (nullable = true)\n",
      " |-- Frequency of Purchases_encoded: vector (nullable = true)\n",
      " |-- num_features: vector (nullable = true)\n",
      " |-- scaled_features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import MinMaxScaler, VectorAssembler\n",
    "\n",
    "# Assemble numeric features for scaling\n",
    "numeric_features = [\"Purchase Amount (USD)\", \"Review Rating\"]\n",
    "assembler = VectorAssembler(inputCols=numeric_features, outputCol=\"num_features\")\n",
    "\n",
    "# Apply Min-Max Scaling\n",
    "scaler = MinMaxScaler(inputCol=\"num_features\", outputCol=\"scaled_features\")\n",
    "\n",
    "pipeline = Pipeline(stages=[assembler, scaler])\n",
    "processed_df = pipeline.fit(processed_df).transform(processed_df)\n",
    "\n",
    "# Drop original numeric columns after scaling\n",
    "processed_df = processed_df.drop(*numeric_features)\n",
    "\n",
    "print(\"âœ… Numeric Feature Scaling Completed!\")\n",
    "processed_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Features Assembled & Unnecessary Columns Dropped!\n",
      "root\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Subscription Status: integer (nullable = true)\n",
      " |-- Discount Applied: integer (nullable = true)\n",
      " |-- Promo Code Used: integer (nullable = true)\n",
      " |-- Previous Purchases: integer (nullable = true)\n",
      " |-- High Value Customer: string (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Selecting encoded categorical & scaled numeric features\n",
    "feature_cols = [\n",
    "    \"Item Purchased_encoded\", \"Category_encoded\", \"Location_encoded\", \"Size_encoded\", \n",
    "    \"Color_encoded\", \"Season_encoded\", \"Payment Method_encoded\", \"Shipping Type_encoded\", \n",
    "    \"Preferred Payment Method_encoded\", \"Frequency of Purchases_encoded\", \"scaled_features\"\n",
    "]\n",
    "\n",
    "# Assemble all features into one vector\n",
    "feature_assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "\n",
    "# Transform data\n",
    "processed_df = feature_assembler.transform(processed_df)\n",
    "\n",
    "# Drop redundant columns\n",
    "columns_to_drop = [\n",
    "    \"Customer ID\", \"Gender\", \"Item Purchased_index\", \"Category_index\", \"Location_index\", \"Size_index\",\n",
    "    \"Color_index\", \"Season_index\", \"Payment Method_index\", \"Shipping Type_index\",\n",
    "    \"Preferred Payment Method_index\", \"Frequency of Purchases_index\", \n",
    "    \"Item Purchased_encoded\", \"Category_encoded\", \"Location_encoded\", \"Size_encoded\", \"Color_encoded\",\n",
    "    \"Season_encoded\", \"Payment Method_encoded\", \"Shipping Type_encoded\", \"Preferred Payment Method_encoded\",\n",
    "    \"Frequency of Purchases_encoded\", \"scaled_features\", \"num_features\"\n",
    "]\n",
    "\n",
    "processed_df = processed_df.drop(*columns_to_drop)\n",
    "\n",
    "print(\"âœ… Features Assembled & Unnecessary Columns Dropped!\")\n",
    "processed_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.classification import RandomForestClassifier, LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n",
    "from pyspark.ml import Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Data Split Completed: Train (3177), Test (723)\n"
     ]
    }
   ],
   "source": [
    "# âœ… Convert target column to numeric (needed for ML)\n",
    "indexer = StringIndexer(inputCol=\"High Value Customer\", outputCol=\"label\")\n",
    "processed_df = indexer.fit(processed_df).transform(processed_df).drop(\"High Value Customer\")\n",
    "\n",
    "# âœ… Train-Test Split (80-20)\n",
    "train_data, test_data = processed_df.randomSplit([0.8, 0.2], seed=42)\n",
    "print(f\"âœ… Data Split Completed: Train ({train_data.count()}), Test ({test_data.count()})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your classifiers\n",
    "models = {\n",
    "    \"Random Forest\": RandomForestClassifier(featuresCol=\"features\", labelCol=\"label\", numTrees=50),\n",
    "    \"Logistic Regression\": LogisticRegression(featuresCol=\"features\", labelCol=\"label\", maxIter=20),\n",
    "}\n",
    "\n",
    "# Define evaluation metrics\n",
    "evaluators = {\n",
    "    \"Accuracy\": MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\"),\n",
    "    \"Precision\": MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedPrecision\"),\n",
    "    \"Recall\": MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedRecall\"),\n",
    "    \"F1 Score\": MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\"),\n",
    "    \"AUC\": BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”¹ Random Forest Performance Metrics:\n",
      "âœ… Accuracy: 0.9281\n",
      "âœ… Precision: 0.9345\n",
      "âœ… Recall: 0.9281\n",
      "âœ… F1 Score: 0.9240\n",
      "âœ… AUC: 0.9991\n",
      "\n",
      "ðŸ”¹ Logistic Regression Performance Metrics:\n",
      "âœ… Accuracy: 0.9723\n",
      "âœ… Precision: 0.9723\n",
      "âœ… Recall: 0.9723\n",
      "âœ… F1 Score: 0.9723\n",
      "âœ… AUC: 0.9974\n"
     ]
    }
   ],
   "source": [
    "# Define a function to train, evaluate, and optionally save predictions.\n",
    "def evaluate_model(model_name, model, train_data, test_data, save_path=None):\n",
    "    \"\"\"\n",
    "    Trains a model using a pipeline, evaluates performance metrics, and saves predictions if save_path is provided.\n",
    "    Returns a tuple: (model_name, Accuracy, Precision, Recall, F1 Score, AUC)\n",
    "    \"\"\"\n",
    "    pipeline = Pipeline(stages=[model])\n",
    "    trained_model = pipeline.fit(train_data)\n",
    "    predictions = trained_model.transform(test_data)\n",
    "    \n",
    "    # Evaluate using each evaluator\n",
    "    metrics = {metric_name: evaluator.evaluate(predictions) \n",
    "               for metric_name, evaluator in evaluators.items()}\n",
    "    \n",
    "    print(f\"\\nðŸ”¹ {model_name} Performance Metrics:\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"âœ… {metric}: {value:.4f}\")\n",
    "    \n",
    "    if save_path is not None:\n",
    "        # Create a filename using the model name (replace spaces with underscores)\n",
    "        filename = f\"{model_name.replace(' ', '_')}_predictions.parquet\"\n",
    "        full_path = f\"{save_path}/{filename}\"\n",
    "        predictions.write.mode(\"overwrite\").parquet(full_path)\n",
    "        print(f\"âœ… Predictions for {model_name} saved to: {full_path}\")\n",
    "    \n",
    "    return (model_name, *metrics.values())\n",
    "\n",
    "# First, evaluate models without saving predictions.\n",
    "model_results = [evaluate_model(name, model, train_data, test_data) for name, model in models.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Model Comparison Results:\n",
      "                 Model  Accuracy  Precision    Recall  F1 Score       AUC\n",
      "0        Random Forest  0.928077   0.934460  0.928077  0.924011  0.999069\n",
      "1  Logistic Regression  0.972337   0.972337  0.972337  0.972337  0.997384\n",
      "âœ… Model comparison results saved to model_comparison_results.csv\n",
      "\n",
      "ðŸ”¹ Best model based on AUC is: Random Forest\n",
      "\n",
      "ðŸ”¹ Random Forest Performance Metrics:\n",
      "âœ… Accuracy: 0.9281\n",
      "âœ… Precision: 0.9345\n",
      "âœ… Recall: 0.9281\n",
      "âœ… F1 Score: 0.9240\n",
      "âœ… AUC: 0.9991\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o2676.parquet.\n: java.lang.UnsatisfiedLinkError: 'boolean org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(java.lang.String, int)'\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793)\r\n\tat org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1249)\r\n\tat org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1454)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getAllCommittedTaskPaths(FileOutputCommitter.java:334)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:404)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)\r\n\tat org.apache.parquet.hadoop.ParquetOutputCommitter.commitJob(ParquetOutputCommitter.java:48)\r\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:192)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeAndCommit$3(FileFormatWriter.scala:275)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:275)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:243)\r\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:802)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 21\u001b[0m\n\u001b[0;32m     18\u001b[0m save_predictions_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/tmp/predictions\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# For local execution, change if needed\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Re-run evaluation for the best model with predictions saved to the specified path\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m best_model_result \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbest_model_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbest_model_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_predictions_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[21], line 23\u001b[0m, in \u001b[0;36mevaluate_model\u001b[1;34m(model_name, model, train_data, test_data, save_path)\u001b[0m\n\u001b[0;32m     21\u001b[0m     filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_predictions.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     22\u001b[0m     full_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msave_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 23\u001b[0m     \u001b[43mpredictions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moverwrite\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfull_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mâœ… Predictions for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m saved to: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfull_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (model_name, \u001b[38;5;241m*\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mvalues())\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Customer_Purchase_Analytics\\lib\\site-packages\\pyspark\\sql\\readwriter.py:1721\u001b[0m, in \u001b[0;36mDataFrameWriter.parquet\u001b[1;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[0;32m   1719\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpartitionBy(partitionBy)\n\u001b[0;32m   1720\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(compression\u001b[38;5;241m=\u001b[39mcompression)\n\u001b[1;32m-> 1721\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Customer_Purchase_Analytics\\lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Customer_Purchase_Analytics\\lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Customer_Purchase_Analytics\\lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o2676.parquet.\n: java.lang.UnsatisfiedLinkError: 'boolean org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(java.lang.String, int)'\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793)\r\n\tat org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1249)\r\n\tat org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1454)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getAllCommittedTaskPaths(FileOutputCommitter.java:334)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:404)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)\r\n\tat org.apache.parquet.hadoop.ParquetOutputCommitter.commitJob(ParquetOutputCommitter.java:48)\r\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:192)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeAndCommit$3(FileFormatWriter.scala:275)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:275)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:243)\r\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:802)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\n"
     ]
    }
   ],
   "source": [
    "# Convert the Python list of model results to a Pandas DataFrame for easier comparison\n",
    "import pandas as pd\n",
    "\n",
    "results_pd = pd.DataFrame(model_results, columns=[\"Model\", *evaluators.keys()])\n",
    "print(\"âœ… Model Comparison Results:\")\n",
    "print(results_pd)\n",
    "\n",
    "# (Optional) Save the comparison results to a CSV file for record-keeping\n",
    "results_pd.to_csv(\"model_comparison_results.csv\", index=False)\n",
    "print(\"âœ… Model comparison results saved to model_comparison_results.csv\")\n",
    "\n",
    "# Select the best model based on the highest AUC\n",
    "best_model_row = results_pd.loc[results_pd[\"AUC\"].idxmax()]\n",
    "best_model_name = best_model_row[\"Model\"]\n",
    "print(f\"\\nðŸ”¹ Best model based on AUC is: {best_model_name}\")\n",
    "\n",
    "# Define the path to save predictions (adjust for local or IBM Cloud Watson Studio)\n",
    "save_predictions_path = \"/tmp/predictions\"  # For local execution, change if needed\n",
    "\n",
    "# Re-run evaluation for the best model with predictions saved to the specified path\n",
    "best_model_result = evaluate_model(best_model_name, models[best_model_name], train_data, test_data, save_path=save_predictions_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
